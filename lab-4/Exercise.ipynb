{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "104a8c32-cc9c-4c25-9651-632f0a7acc0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/23ae2440-d9b7-4bf1-b490-\n",
      "[nltk_data]     439ad9e57e22/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/23ae2440-d9b7-4bf1-b490-\n",
      "[nltk_data]     439ad9e57e22/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /home/23ae2440-d9b7-4bf1-b490-\n",
      "[nltk_data]     439ad9e57e22/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/23ae2440-d9b7-4bf1-b490-\n",
      "[nltk_data]     439ad9e57e22/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/23ae2440-d9b7-4bf1-b490-\n",
      "[nltk_data]     439ad9e57e22/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review  \\\n",
      "0  Im happy with uniten actually, even the people...   \n",
      "1  I’m having a pretty good time here, happy to m...   \n",
      "2        a very neutral place in terms of everything   \n",
      "3  I would say Uniten it's  a good university  bu...   \n",
      "4   UNITEN is well-regarded, particularly for its...   \n",
      "\n",
      "                                           processed  \n",
      "0     [im, happy, uniten, actually, even, people, w]  \n",
      "1   [im, pretty, good, time, happy, meet, w, people]  \n",
      "2                 [neutral, place, term, everything]  \n",
      "3  [would, say, uniten, good, university, issue, ...  \n",
      "4  [uniten, wellregarded, particularly, strong, e...  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "import string\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "from autocorrect import Speller\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "\n",
    "# =========================\n",
    "# Download Required NLTK Data\n",
    "# =========================\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# =========================\n",
    "# Initialize Tools\n",
    "# =========================\n",
    "spell = Speller(lang='en')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# IMPORTANT:\n",
    "# Keep important domain words\n",
    "custom_keep_words = {\n",
    "    \"uniten\", \"ucc\", \"dss\", \"bm\", \"bv\",\n",
    "    \"wifi\", \"parking\", \"lecturer\",\n",
    "    \"management\", \"hostel\", \"library\"\n",
    "}\n",
    "\n",
    "stop_words = stop_words - custom_keep_words\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# =========================\n",
    "# Fix Encoding Issues (â€™ etc.)\n",
    "# =========================\n",
    "def fix_encoding(text):\n",
    "    if isinstance(text, str):\n",
    "        return text.encode('latin1', errors='ignore').decode('utf-8', errors='ignore')\n",
    "    return text\n",
    "\n",
    "# =========================\n",
    "# Remove URLs\n",
    "# =========================\n",
    "def remove_urls(text):\n",
    "    return re.sub(r'http\\S+|www\\S+', '', text)\n",
    "\n",
    "# =========================\n",
    "# Remove HTML\n",
    "# =========================\n",
    "def remove_html(text):\n",
    "    return BeautifulSoup(text, \"html.parser\").get_text()\n",
    "\n",
    "# =========================\n",
    "# Remove Emojis\n",
    "# =========================\n",
    "def remove_emojis(text):\n",
    "    return emoji.replace_emoji(text, replace='')\n",
    "\n",
    "# =========================\n",
    "# Remove Punctuation\n",
    "# =========================\n",
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "# =========================\n",
    "# Remove Numbers\n",
    "# =========================\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "# =========================\n",
    "# Remove Stopwords\n",
    "# =========================\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word not in stop_words]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "# =========================\n",
    "# POS Mapping\n",
    "# =========================\n",
    "def get_wordnet_pos(nltk_tag):\n",
    "    if nltk_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif nltk_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif nltk_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif nltk_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "# =========================\n",
    "# Lemmatization\n",
    "# =========================\n",
    "def lemmatize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    words = word_tokenize(text)\n",
    "    pos_tags = pos_tag(words)\n",
    "\n",
    "    lemmatized_words = [\n",
    "        lemmatizer.lemmatize(word, get_wordnet_pos(tag))\n",
    "        for word, tag in pos_tags\n",
    "    ]\n",
    "\n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "# =========================\n",
    "# Tokenization\n",
    "# =========================\n",
    "def tokenize_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# =========================\n",
    "# FULL PREPROCESSING PIPELINE\n",
    "# =========================\n",
    "def preprocess_text(text):\n",
    "\n",
    "    if not isinstance(text, str):\n",
    "        return []\n",
    "\n",
    "    text = fix_encoding(text)          # Fix encoding issues\n",
    "    text = text.lower()                # Lowercase\n",
    "    text = remove_urls(text)\n",
    "    text = remove_html(text)\n",
    "    text = remove_emojis(text)\n",
    "    text = remove_punctuation(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = lemmatize_text(text)\n",
    "    tokens = tokenize_text(text)\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# =========================\n",
    "# LOAD DATASET\n",
    "# =========================\n",
    "df = pd.read_csv(\"UNITENReview.csv\")\n",
    "\n",
    "# Drop rows like \"#NAME?\"\n",
    "df = df[df[\"Review\"].str.contains(\"#NAME?\", na=False) == False]\n",
    "\n",
    "# Apply preprocessing\n",
    "df[\"processed\"] = df[\"Review\"].apply(preprocess_text)\n",
    "\n",
    "# Save processed dataset\n",
    "df.to_csv(\"Processed_UNITEN_Reviews.csv\", index=False)\n",
    "\n",
    "# Display sample\n",
    "print(df[[\"Review\", \"processed\"]].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1a36c7-3b59-425d-a857-496b6f569a62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2025.12-py312",
   "language": "python",
   "name": "conda-env-anaconda-2025.12-py312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
